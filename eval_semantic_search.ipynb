{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:23:39.574300800Z",
     "start_time": "2023-12-24T21:23:34.348914700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "from typing import Callable\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def semantic_search(query_embeddings: Tensor,\n",
    "                    corpus_embeddings: Tensor,\n",
    "                    query_chunk_size: int = 100,\n",
    "                    corpus_chunk_size: int = 500000,\n",
    "                    top_k: int = 10,\n",
    "                    score_function: Callable[[Tensor, Tensor], Tensor] = util.cos_sim):\n",
    "    \"\"\"\n",
    "    This function performs a cosine similarity search between a list of query embeddings  and a list of corpus embeddings.\n",
    "    It can be used for Information Retrieval / Semantic Search for corpora up to about 1 Million entries.\n",
    "\n",
    "    :param query_embeddings: A 2 dimensional tensor with the query embeddings.\n",
    "    :param corpus_embeddings: A 2 dimensional tensor with the corpus embeddings.\n",
    "    :param query_chunk_size: Process 100 queries simultaneously. Increasing that value increases the speed, but requires more memory.\n",
    "    :param corpus_chunk_size: Scans the corpus 100k entries at a time. Increasing that value increases the speed, but requires more memory.\n",
    "    :param top_k: Retrieve top k matching entries.\n",
    "    :param score_function: Function for computing scores. By default, cosine similarity.\n",
    "    :return: Returns a list with one entry for each query. Each entry is a list of dictionaries with the keys 'corpus_id' and 'score', sorted by decreasing cosine similarity scores.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(query_embeddings, (np.ndarray, np.generic)):\n",
    "        query_embeddings = torch.from_numpy(query_embeddings)\n",
    "    elif isinstance(query_embeddings, list):\n",
    "        query_embeddings = torch.stack(query_embeddings)\n",
    "\n",
    "    if len(query_embeddings.shape) == 1:\n",
    "        query_embeddings = query_embeddings.unsqueeze(0)\n",
    "\n",
    "    if isinstance(corpus_embeddings, (np.ndarray, np.generic)):\n",
    "        corpus_embeddings = torch.from_numpy(corpus_embeddings)\n",
    "    elif isinstance(corpus_embeddings, list):\n",
    "        corpus_embeddings = torch.stack(corpus_embeddings)\n",
    "\n",
    "\n",
    "    #Check that corpus and queries are on the same device\n",
    "    if corpus_embeddings.device != query_embeddings.device:\n",
    "        query_embeddings = query_embeddings.to(corpus_embeddings.device)\n",
    "\n",
    "    queries_result_list = [[] for _ in range(len(query_embeddings))]\n",
    "\n",
    "    for query_start_idx in tqdm(range(0, len(query_embeddings), query_chunk_size)):\n",
    "        # Iterate over chunks of the corpus\n",
    "        for corpus_start_idx in range(0, len(corpus_embeddings), corpus_chunk_size):\n",
    "            # Compute cosine similarities\n",
    "            cos_scores = score_function(query_embeddings[query_start_idx:query_start_idx+query_chunk_size], corpus_embeddings[corpus_start_idx:corpus_start_idx+corpus_chunk_size])\n",
    "\n",
    "            # Get top-k scores\n",
    "            cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(cos_scores, min(top_k, len(cos_scores[0])), dim=1, largest=True, sorted=False)\n",
    "            cos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()\n",
    "            cos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()\n",
    "\n",
    "            for query_itr in range(len(cos_scores)):\n",
    "                for sub_corpus_id, score in zip(cos_scores_top_k_idx[query_itr], cos_scores_top_k_values[query_itr]):\n",
    "                    corpus_id = corpus_start_idx + sub_corpus_id\n",
    "                    query_id = query_start_idx + query_itr\n",
    "                    queries_result_list[query_id].append((corpus_id, score))\n",
    "\n",
    "    #Sort and strip to top_k results\n",
    "    for i, sublist in enumerate(queries_result_list):\n",
    "        queries_result_list[i] = sorted(sublist, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    return queries_result_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:23:39.580497400Z",
     "start_time": "2023-12-24T21:23:39.579337600Z"
    }
   },
   "id": "9f66db9f6e9d10f1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "queries = pd.read_csv(r\"queries.doctrain.tsv\", sep='\\t', header=None, names=['qid', 'query'], index_col=['qid'])['query']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:23:39.767926900Z",
     "start_time": "2023-12-24T21:23:39.581530800Z"
    }
   },
   "id": "2dd00b7599dc9196"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "query_list = queries.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:23:39.773925800Z",
     "start_time": "2023-12-24T21:23:39.767926900Z"
    }
   },
   "id": "b69289fa5138fbd4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\n",
    "model_name = 'msmarco-MiniLM-L6-cos-v5'\n",
    "bi_encoder = SentenceTransformer(model_name, device=device)\n",
    "bi_encoder.max_seq_length = 256     #Truncate long passages to 256 tokens\n",
    "top_k = 32                          #Number of passages we want to retrieve with the bi-encoder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:23:40.394227200Z",
     "start_time": "2023-12-24T21:23:39.779230700Z"
    }
   },
   "id": "1b67f14291c1b93c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/11470 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "018966b4041f482b90356e648fa74e96"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question_embedding = bi_encoder.encode(query_list, convert_to_tensor=True, show_progress_bar=True, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:24:33.291702400Z",
     "start_time": "2023-12-24T21:23:40.395252900Z"
    }
   },
   "id": "b77c3edaa011d3a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "corpus_embeddings = torch.load(f'corpus_embeddings_{model_name}.pt', map_location=device, mmap=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:24:35.022309800Z",
     "start_time": "2023-12-24T21:24:33.292732300Z"
    }
   },
   "id": "a87c4f1df2420379"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3671/3671 [08:47<00:00,  6.95it/s]\n"
     ]
    }
   ],
   "source": [
    "results = semantic_search(question_embedding, corpus_embeddings, top_k=top_k)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:33:30.746557Z",
     "start_time": "2023-12-24T21:24:35.022309800Z"
    }
   },
   "id": "2c51eabe722e9267"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "for i, (qid, result) in enumerate(zip(queries.index, results)):\n",
    "    results[i] = [(qid, r[0]) for r in result]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:33:31.839269900Z",
     "start_time": "2023-12-24T21:33:30.747987500Z"
    }
   },
   "id": "858033d5df1db77a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# save results\n",
    "with open(f'semantic_search_results_{model_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:33:35.467031Z",
     "start_time": "2023-12-24T21:33:31.842153500Z"
    }
   },
   "id": "83803729be14b533"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "with open('corpus_id_to_doc_id.txt') as f:\n",
    "    doc_ids = list(map(int, f.read().splitlines()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:33:35.861398600Z",
     "start_time": "2023-12-24T21:33:35.468119200Z"
    }
   },
   "id": "db75bfdf693a47d4"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "pid\n<class 'int'>    367013\nName: count, dtype: int64"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance = pd.read_csv(r\"msmarco-doctrain-qrels-idconverted.tsv\", sep='\\t', header=None, names=['qid', 'pid'], index_col=['qid'], dtype=int)['pid']\n",
    "relevance.map(type).value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:33:35.954242100Z",
     "start_time": "2023-12-24T21:33:35.862419900Z"
    }
   },
   "id": "b91c3904a40d3ab"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "n_results = 10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:33:35.954242100Z",
     "start_time": "2023-12-24T21:33:35.946300900Z"
    }
   },
   "id": "9f3a2530a7732ee1"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "367013it [00:03, 100236.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.3147422776294608"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pak_sum = 0\n",
    "for qid, result in tqdm(zip(queries.index, results)):\n",
    "    for i in range(n_results):\n",
    "        if relevance[qid] == doc_ids[result[i][1]]:\n",
    "            pak_sum += 1 / (i + 1)\n",
    "            break\n",
    "pak_sum / len(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:33:39.630606300Z",
     "start_time": "2023-12-24T21:33:35.949939800Z"
    }
   },
   "id": "84aec2bf0a1b340b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-24T21:33:39.631722600Z",
     "start_time": "2023-12-24T21:33:39.622743400Z"
    }
   },
   "id": "5cb7403c92139e17",
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
